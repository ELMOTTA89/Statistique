{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Projet : Consommation et production électrique en France"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    },
    "tags": []
   },
   "source": [
    "<img width=400 src=\"https://1.bp.blogspot.com/-_cw5mPFrxmc/XtuawSHRV0I/AAAAAAAAEWQ/52ff8l3-MKI0_ZdlJpwwyrH6tgh9diaOQCLcBGAsYHQ/s1600/uses-of-electricity-in-our-daily-life.jpg\">\n",
    "<p style=\"text-align: left\"> Une  photo kitsch du réseau électrique </p>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Présentation Du Dataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "\n",
    "<img src=\"./assets/image1.png\" width=\"300\"/>\n",
    "<img src=\"./assets/image2.png\" width=\"300\"/>\n",
    "\n",
    "**Informations issues du site (allez voir vous même également) :**\n",
    "\n",
    "Ce jeu de données, rafraîchi une fois par jour, présente les données régionales de janvier 2013 à 2023. Elles sont issues de l'application éCO2mix. Elles sont élaborées à partir des comptages et complétées par des forfaits. \n",
    "\n",
    "Vous y trouverez au pas quart d'heure :\n",
    "\n",
    "- Les prévisions de consommation établies la veille (J-1) et celles réactualisées le jour même (J).\n",
    "\n",
    "Vous y trouverez au pas demi-heure :\n",
    "\n",
    "- La consommation réalisée.\n",
    "- La production selon les différentes filières composant le mix énergétique.\n",
    "- La consommation des pompes dans les Stations de Transfert d'Energie par Pompage (STEP).\n",
    "- Les échanges physiques aux frontières.\n",
    "- Une estimation des émissions de carbone générées par la production d'électricité en France.\n",
    "- Les échanges commerciaux aux frontières.\n",
    "- Le découpage des filières par technologie du mix de production (débute en 2013).\n",
    "\n",
    "\n",
    "\n",
    "<img src=\"./assets/image3.png\" width=\"800\"/>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Objectifs et modalités de l'étude"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Modalités de l'étude :** \n",
    "\n",
    "- Vous travaillerez en groupe de 2 à 3.    \n",
    "   \n",
    "- Une présentation de votre travail sur une question sera effectuée en fin du projet. \n",
    "   \n",
    "**Les objectifs de cette étude sont multiples :** \n",
    "\n",
    "- Apprendre à charger et manipuler des données réelles complexes avec Pandas. \n",
    "\n",
    "- Manipuler des séries temporelles. \n",
    "\n",
    "- Analyser des données pour répondre à une question exploratoire. \n",
    "\n",
    "- Présenter et vulgariser votre recherche exploratoire. \n",
    "\n",
    "**Notes sur les données RTE** :\n",
    "\n",
    "- Elles proviennent du (génial) site éCO2mix et sont disponibles pour tout le monde (opendata) : https://www.rte-france.com/eco2mix \n",
    "- Données agrégées au niveau national : https://opendata.reseaux-energies.fr/explore/dataset/eco2mix-national-cons-def/information/?disjunctive.nature \n",
    "- Données agrégées au niveau régional : https://opendata.reseaux-energies.fr/explore/dataset/eco2mix-regional-cons-def/information/?disjunctive.libelle_region&disjunctive.nature"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "jp-MarkdownHeadingCollapsed": true
   },
   "source": [
    "## Conseils & Remarques sur l'exploration des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "- La première étape de chargement, exploration et nettoyage de données peut être chronophage lorsqu'on traite des sets de données **réels**... Cela fait partie du travail de data scientist, il faut s'y faire :-)\n",
    "\n",
    "> It takes less than five lines of code to train a basic machine learning algorithm. Exploratory data analysis and data preparation in comparison take longer and comprise of 80% of the data scientist’s time.\"  https://towardsdatascience.com/build-the-story-around-data-using-exploratory-data-analysis-and-pandas-c85bf3beff87\n",
    "\n",
    "- Quand vous faîtes des recherches et que vous manipulez les données, n'oubliez pas de clarifier : **Quelle question/ hypothèse essayez-vous de résoudre/de prouver/ d'invalider ?**\n",
    "\n",
    "- Votre notebook doit être **compréhensible**. Il doit vous permettre de partager vos recherches. Le lecteur final doit pouvoir le lire comme une histoire (collègue, vous dans un futur proche, etc.). Utilisez du **markdown** pour commenter votre code, discuter des résultats, insérer des images, ...\n",
    "\n",
    "- **Table of Content** : mettez vous une table des matières et activez le **synchronize collapse state**. Ca vous permettre de vous y retrouver plus facilement. \n",
    "\n",
    "   \n",
    "- Garantissez la **causalité** de votre notebook : l'ordre d'exécution des cellules dans un notebook est complexe. Ne faîtes pas l'erreur de ne pas vérifier que vous pouvez exécuter l'ensemble de vos cellules dans l'ordre. Sinon vous n'arriverez plus à exécuter votre notebook. \n",
    "\n",
    "- N'oubliez pas de reprendre les étapes d'exploration classiques des données vues précédemment (projets GapMinder, Arbres de Grenoble, ...) : afficher les informations sur vos dataframes, regardez les données, faites des sauvegardes intermédiaires (format .pkl par exemple), cherchez les outliers, les données manquantes, etc... \n",
    "\n",
    "- C'est une analyse exploratoire : **tatonnez**, **faîtes des graphiques**, ... \n",
    "\n",
    "\n",
    "- Lorsque c'est nécessaire (selon ce que vous cherchez) penser à normaliser/standardiser les données\n",
    "\n",
    "- N'hésitez pas à **consulter l'aide de Pandas** ou à chercher la réponse à vos questions sur internet (quasiment tous les bugs que vous observerez on déjà fait l'objet d'un post sur **stackoverflow**)\n",
    "\n",
    "**Ressources** :\n",
    "-  Markdown : https://jupyter-notebook.readthedocs.io/en/stable/examples/Notebook/Working%20With%20Markdown%20Cells.html\n",
    "-  Table of content : https://jupyterlab.readthedocs.io/en/stable/user/toc.html. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Chargement et préparation des données"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x1F4A5; **To Do** \n",
    "\n",
    "- Charger les données nationales.\n",
    " \n",
    "- Regarder les colonnes, sélectionner les données intéressantes.\n",
    " \n",
    "- Les types inférés semblent-ils corrects ? Corriger si nécessaire (datetime, object, int, float, etc.)\n",
    "\n",
    "- Choisir des noms de colonnes plus faciles à manipuler (espaces, accents...). <br/>Ressource : https://www.dataschool.io/pandas-dot-notation-vs-brackets/\n",
    "\n",
    "- Choisir un index adéquat pour votre dataframe. Les lignes ont un comportement étrange, choisissez les lignes qui vous arrangent.\n",
    "\n",
    "- Quelle période temporelle couvrent les données ?\n",
    "\n",
    "- Faîtes un choix par rapport aux valeurs manquantes. Les garder sous forme de NaN ou une autre valeur ?\n",
    "\n",
    "- Simplifiez le DataFrame : regardez vos colonnnes, cherchez à les comprendre et rassemblez celles que vous pouvez rassembler. \n",
    "\n",
    "- Sauvegardez vos données sous format **pkl** pour ne pas devoir refaire les pré-traitements à chaque fois que vous redémarrez le kernel.  \n",
    "\n",
    "\n",
    "&#x1F4A5; **Ressources**\n",
    "\n",
    "- La doc de pandas.\n",
    "\n",
    "- Voici quelques fonctions en vrac dont vous aurez besoin (read_csv,  info,  drop, to_datetime, astype, nunique, set_index)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# importer les modules et les packages\n",
    "import numpy as np\n",
    "import pandas as pd # type: ignore\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib.dates as dat\n",
    "import datetime as dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# définir les fonctions souvent appelées\n",
    "def print_horizontal_line():\n",
    "    print(\"\\u2500\" * 120)\n",
    "\n",
    "def plot_stat_curves(national_df: pd.DataFrame, year_mask: bool, dark_colour: str, lite_colour: str, axe: plt.Axes | np.ndarray):\n",
    "    weekly_x0 = national_df.loc[year_mask].resample(\"W\")[\"Date et Heure\"].mean()\n",
    "    weekly_y1 = national_df.loc[year_mask].resample(\"W\")[\"Consommation (MW)\"].mean()\n",
    "    weekly_y2 = national_df.loc[year_mask].resample(\"W\")[\"Consommation (MW)\"].std()\n",
    "    axe.plot(weekly_x0, (weekly_y1 / 1000.0), color = dark_colour, linewidth = 3.0)\n",
    "    axe.fill_between(weekly_x0, (weekly_y1 - weekly_y2) / 1000.0, (weekly_y1 + weekly_y2) / 1000.0, color = lite_colour, alpha = 0.5)\n",
    "\n",
    "def calculate_autonomy(national_df: pd.DataFrame, date_mask: bool) -> pd.Series:\n",
    "    total_autonomy = 0.0\n",
    "    total_autonomy += national_df.loc[date_mask].resample(\"ME\")[\"Bioénergies (MW)\"].mean()\n",
    "    total_autonomy += national_df.loc[date_mask].resample(\"ME\")[\"Charbon (MW)\"].mean()\n",
    "    total_autonomy += national_df.loc[date_mask].resample(\"ME\")[\"Eolien (MW)\"].mean()\n",
    "    total_autonomy += national_df.loc[date_mask].resample(\"ME\")[\"Fioul (MW)\"].mean()\n",
    "    total_autonomy += national_df.loc[date_mask].resample(\"ME\")[\"Gaz (MW)\"].mean()\n",
    "    total_autonomy += national_df.loc[date_mask].resample(\"ME\")[\"Hydraulique (MW)\"].mean()\n",
    "    total_autonomy += national_df.loc[date_mask].resample(\"ME\")[\"Nucléaire (MW)\"].mean()\n",
    "    total_autonomy += national_df.loc[date_mask].resample(\"ME\")[\"Solaire (MW)\"].mean()\n",
    "    total_autonomy += national_df.loc[date_mask].resample(\"ME\")[\"Pompage (MW)\"].mean()\n",
    "    total_autonomy += national_df.loc[date_mask].resample(\"ME\")[\"Ech. physiques (MW)\"].mean()\n",
    "    total_autonomy -= national_df.loc[date_mask].resample(\"ME\")[\"Consommation (MW)\"].mean()\n",
    "    return total_autonomy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# ajuster des options d'affichage (ici toutes les colonnes des tableaux seront visibles et défilables)\n",
    "pd.set_option('display.max_columns', None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_data_frame = pd.read_csv(\"data/eco2mix-national-cons-def.csv\", sep = \";\", header = 0, names = None, index_col = False, dtype = str, engine = \"python\")\n",
    "print_horizontal_line() # type: ignore\n",
    "print(\"Informations globales :\")\n",
    "print(national_data_frame.info())\n",
    "# Toutes les données brutes sont des objets : il faudra les convertir pour les traiter. En plus des données manquent...\n",
    "print_horizontal_line() # type: ignore\n",
    "print(\"Description brute :\")\n",
    "print(national_data_frame.describe())\n",
    "print_horizontal_line() # type: ignore\n",
    "# Nous estimons la taille du tableau et nous parcourons toutes ses colonnes (ses champs) :\n",
    "national_data_frame = national_data_frame.sort_values(by = [\"Date et Heure\"])\n",
    "print(\"Ce data-frame sur les données nationales compte\", national_data_frame.shape[0], \"lignes et\", national_data_frame.shape[1], \"colonnes :\")\n",
    "for national_column in national_data_frame.columns:\n",
    "    # Nous comptons les lignes auxquelles des données manquent, puis nous écrivons un pourcentage relatif : \n",
    "    not_applicable_rows_quantity = national_data_frame[national_column].isna().sum()\n",
    "    print(national_column, \": rempli à\", round(100.0 - (100.0 * float(not_applicable_rows_quantity) / float(national_data_frame.shape[0])), 1), \"%\")\n",
    "    if national_column == \"Périmètre\":\n",
    "        # Les données de la colonne périmètre (France) ne nous intéressent pas : nous les enlevons !\n",
    "        national_data_frame.drop(national_column, axis = 1, inplace = True)\n",
    "    elif national_column == \"Nature\":\n",
    "        # Les données de la colonne nature (données définitives) ne nous intéressent pas : nous les enlevons !\n",
    "        national_data_frame.drop(national_column, axis = 1, inplace = True)\n",
    "    elif national_column == \"Date\":\n",
    "        # Nous convertissons la date locale au format AAAA-MM-JJ, sans horaire :\n",
    "        national_data_frame[national_column] = pd.to_datetime(national_data_frame[national_column], format = \"%Y-%m-%d\").dt.date\n",
    "    elif national_column == \"Heure\":\n",
    "        # Nous convertissons l'horaire local au format HH:MM:SS, sans date :\n",
    "        national_data_frame[national_column] = pd.to_datetime(national_data_frame[national_column], format = \"%H:%M\").dt.time\n",
    "    elif national_column == \"Date et Heure\":\n",
    "        # Nous convertissons la date et l'horaire au format international ISO-8601, calé sur le fuseau horaire d'origine.\n",
    "        national_data_frame[national_column] = pd.to_datetime(national_data_frame[national_column], format = \"%Y-%m-%dT%H:%M:%S%z\", utc = True)\n",
    "    else:\n",
    "        # Les autres colonnes contiennent des nombres entiers : nous filtrons les données manquantes et nous appliquons la bonne taille mémoire :\n",
    "        national_data_frame[national_column] = pd.to_numeric(national_data_frame[national_column], errors = \"coerce\").astype(\"Int32\")\n",
    "        national_data_frame = national_data_frame[national_data_frame[national_column].notna()]\n",
    "print_horizontal_line() # type: ignore\n",
    "national_data_frame.to_pickle(\"data/national-data-frame.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous affichons le tableau nettoyé :\n",
    "national_data_frame"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "realtime_data_frame = pd.read_csv(\"data/eco2mix-national-tr.csv\", sep = \";\", header = 0, names = None, index_col = False, dtype = str, engine = \"python\")\n",
    "print_horizontal_line() # type: ignore\n",
    "print(\"Informations globales :\")\n",
    "print(realtime_data_frame.info())\n",
    "# Toutes les données brutes sont des objets : il faudra les convertir pour les traiter. En plus des données manquent...\n",
    "print_horizontal_line() # type: ignore\n",
    "print(\"Description brute :\")\n",
    "print(realtime_data_frame.describe())\n",
    "print_horizontal_line() # type: ignore\n",
    "# Nous estimons la taille du tableau et nous parcourons toutes ses colonnes (ses champs) :\n",
    "print(\"Ce data-frame sur les données en temps réel compte\", realtime_data_frame.shape[0], \"lignes et\", realtime_data_frame.shape[1], \"colonnes :\")\n",
    "realtime_data_frame.replace(to_replace = \"ND\", value = None)\n",
    "realtime_data_frame.sort_values(by = [\"Date - Heure\"])\n",
    "for realtime_column in realtime_data_frame.columns:\n",
    "    not_applicable_rows_quantity = realtime_data_frame[realtime_column].isna().sum()\n",
    "    print(realtime_column, \": rempli à\", round(100.0 - (100.0 * float(not_applicable_rows_quantity) / float(realtime_data_frame.shape[0])), 1), \"%\")\n",
    "    if realtime_column == \"Périmètre\":\n",
    "        # Les données de la colonne périmètre (France) ne nous intéressent pas : nous les enlevons !\n",
    "        realtime_data_frame.drop(realtime_column, axis = 1, inplace = True)\n",
    "    elif realtime_column == \"Nature\":\n",
    "        # Les données de la colonne nature (données définitives) ne nous intéressent pas : nous les enlevons !\n",
    "        realtime_data_frame.drop(realtime_column, axis = 1, inplace = True)\n",
    "    elif realtime_column == \"Date\":\n",
    "        # Nous convertissons la date locale au format AAAA-MM-JJ, sans horaire :\n",
    "        realtime_data_frame[realtime_column] = pd.to_datetime(realtime_data_frame[realtime_column], format = \"%Y-%m-%d\").dt.date\n",
    "    elif realtime_column == \"Heure\":\n",
    "        # Nous convertissons l'horaire local au format HH:MM:SS, sans date :\n",
    "        realtime_data_frame[realtime_column] = pd.to_datetime(realtime_data_frame[realtime_column], format = \"%H:%M\").dt.time\n",
    "    elif realtime_column == \"Date - Heure\":\n",
    "        # Nous convertissons la date et l'horaire au format international ISO-8601, calé sur le fuseau horaire d'origine.\n",
    "        realtime_data_frame[realtime_column] = pd.to_datetime(realtime_data_frame[realtime_column], format = '%Y-%m-%dT%H:%M:%S%z', utc = True)\n",
    "    else:\n",
    "        # Les autres colonnes contiennent des nombres entiers : nous filtrons les données manquantes et nous appliquons la bonne taille mémoire :\n",
    "        realtime_data_frame[realtime_column] = pd.to_numeric(realtime_data_frame[realtime_column], errors = \"coerce\").astype(\"Int32\")\n",
    "        realtime_data_frame = realtime_data_frame[realtime_data_frame[realtime_column].notna()]\n",
    "print_horizontal_line() # type: ignore\n",
    "realtime_data_frame.to_pickle(\"data/realtime-data-frame.pkl\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Nous affichons le tableau nettoyé :\n",
    "realtime_data_frame"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Exploration du dataset National"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Réflexion"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "hidden": true
   },
   "source": [
    "Avant de vous lancer dans la mise en pratique, prenez quelques minutes (max 30 minutes) pour observer les données et explorer les premières statistiques descriptives : \n",
    "- Quelles questions vous posez-vous sur les données ? \n",
    "\n",
    "- Qu'avez-vous envie d'explorer ? Soyez créatifs !"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "national_df = pd.read_pickle(\"data/national-data-frame.pkl\")\n",
    "date_time_index = pd.DatetimeIndex(data = national_df[\"Date et Heure\"], dtype=\"datetime64[ns, UTC]\")\n",
    "national_df.set_index(date_time_index, inplace = True)\n",
    "realtime_df = pd.read_pickle(\"data/realtime-data-frame.pkl\")\n",
    "\n",
    "dark_colours = [\"#CC3300\", \"#CC9900\", \"#669900\", \"#33CC66\", \"#00CC33\", \"#0099CC\", \"#3366CC\", \"#6633CC\", \"#993399\", \"#CC0033\"]\n",
    "lite_colours = [\"#FF6633\", \"#FFCC33\", \"#99CC33\", \"#66FF99\", \"#33FF66\", \"#33CCFF\", \"#6699FF\", \"#9966FF\", \"#CC66CC\", \"#FF3366\"]\n",
    "\n",
    "fig1, axe1 = plt.subplots(nrows = 10, ncols = 2, figsize = (20.0, 30.0), gridspec_kw = {\"width_ratios\" : [5 , 1], \"height_ratios\" : [1, 1, 1, 1, 1, 1, 1, 1, 1, 1]})\n",
    "for index, year_as_number in enumerate(range(2013, 2022 + 1, 1)):\n",
    "    year_as_string = str(year_as_number)\n",
    "    year_mask = (national_df[\"Date\"] >= pd.to_datetime(year_as_string + \"-01-01\").date()) & (national_df[\"Date\"] <= pd.to_datetime(year_as_string + \"-12-31\").date())\n",
    "\n",
    "    axe1[index, 0].plot(\n",
    "        national_df[\"Date et Heure\"].loc[year_mask], \n",
    "        national_df[\"Taux de CO2 (g/kWh)\"].loc[year_mask],  # gramme par kWh\n",
    "        color = \"#AAAAAA\", linewidth = 0.5)    \n",
    "    axe1[index, 0].plot(\n",
    "        national_df[\"Date et Heure\"].loc[year_mask], \n",
    "        national_df[\"Consommation (MW)\"].loc[year_mask] / 1000.0, # Giga-Watts\n",
    "        color = lite_colours[index], linewidth = 0.5)\n",
    "\n",
    "    if year_as_number == 2021:\n",
    "        year_mask = (national_df[\"Date\"] >= pd.to_datetime(year_as_string + \"-01-01\").date()) & (national_df[\"Date\"] <= pd.to_datetime(year_as_string + \"-03-15\").date())\n",
    "        plot_stat_curves(national_df, year_mask, dark_colours[index], lite_colours[index], axe1[index, 0])\n",
    "        year_mask = (national_df[\"Date\"] >= pd.to_datetime(year_as_string + \"-04-01\").date()) & (national_df[\"Date\"] <= pd.to_datetime(year_as_string + \"-08-31\").date())\n",
    "        plot_stat_curves(national_df, year_mask, dark_colours[index], lite_colours[index], axe1[index, 0])\n",
    "        year_mask = (national_df[\"Date\"] >= pd.to_datetime(year_as_string + \"-12-01\").date()) & (national_df[\"Date\"] <= pd.to_datetime(year_as_string + \"-12-31\").date())\n",
    "        plot_stat_curves(national_df, year_mask, dark_colours[index], lite_colours[index], axe1[index, 0])\n",
    "    else:\n",
    "        plot_stat_curves(national_df, year_mask, dark_colours[index], lite_colours[index], axe1[index, 0])\n",
    "\n",
    "    if year_as_number == 2022:\n",
    "        axe1[index, 0].set_xticklabels([\"J\", \"F\", \"M\", \"A\", \"M\", \"J\", \"J\", \"A\", \"S\", \"O\", \"N\", \"D\"])\n",
    "    else:\n",
    "        axe1[index, 0].set_xticklabels(\"\")\n",
    "\n",
    "    axe1[index, 0].xaxis.set_major_locator(dat.MonthLocator())\n",
    "    axe1[index, 0].xaxis.set_minor_locator(dat.WeekdayLocator(dat.SUNDAY))\n",
    "    axe1[index, 0].set_title(year_as_string)\n",
    "    axe1[index, 0].grid(color = \"#CCCCCC\", which=\"major\", axis = \"both\", linewidth = 0.5)\n",
    "    axe1[index, 0].grid(color = \"#EEEEEE\", which=\"minor\", axis = \"both\", linewidth = 0.5)\n",
    "    axe1[index, 0].set_xlim([pd.to_datetime(year_as_string + \"-01-01\").date(), pd.to_datetime(year_as_string + \"-12-31\").date()])\n",
    "    axe1[index, 0].set_ylim([0, 120])\n",
    "\n",
    "    pie_chart_labels = [\"B\", \"C\", \"E\", \"F\", \"G\", \"H\", \"N\", \"S\"]\n",
    "    pie_chart_colours = [\"#009900\", \"#666666\", \"#990033\", \"#330099\", \"#003399\", \"#009966\", \"#993300\", \"#666600\"]\n",
    "\n",
    "    pie_chart_quantities = [\n",
    "        national_df[\"Bioénergies (MW)\"].loc[year_mask].sum(),\n",
    "        national_df[\"Charbon (MW)\"].loc[year_mask].sum(),\n",
    "        national_df[\"Eolien (MW)\"].loc[year_mask].sum(),\n",
    "        national_df[\"Fioul (MW)\"].loc[year_mask].sum(),\n",
    "        national_df[\"Gaz (MW)\"].loc[year_mask].sum(),\n",
    "        national_df[\"Hydraulique (MW)\"].loc[year_mask].sum(),\n",
    "        national_df[\"Nucléaire (MW)\"].loc[year_mask].sum(),\n",
    "        national_df[\"Solaire (MW)\"].loc[year_mask].sum()\n",
    "    ]\n",
    "    axe1[index, 1].pie(pie_chart_quantities, labels = pie_chart_labels, colors = pie_chart_colours, startangle = 90)\n",
    "\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig2, axe2 = plt.subplots(nrows = 1, ncols = 2, figsize = (20.0, 10.0), gridspec_kw = {\"width_ratios\" : [2 , 1], \"height_ratios\" : [1]})\n",
    "\n",
    "date_mask_1 = (national_df[\"Date\"] >= pd.to_datetime(\"2017-04-01\").date()) & (national_df[\"Date\"] <= pd.to_datetime(\"2020-09-30\").date())\n",
    "total_autonomy_1 = calculate_autonomy(national_df, date_mask_1)\n",
    "axe2[0].fill_between(national_df.loc[date_mask_1].resample(\"ME\")[\"Date et Heure\"].mean(), 0.0, (total_autonomy_1 / 1000.0) , color = \"#FFCC99\", alpha = 0.5)\n",
    "axe2[0].set_ylim([-0.001, 0.001])\n",
    "\n",
    "date_mask_2 = (national_df[\"Date\"] >= pd.to_datetime(\"2022-01-01\").date()) & (national_df[\"Date\"] <= pd.to_datetime(\"2022-12-31\").date())\n",
    "total_autonomy_2 = calculate_autonomy(national_df, date_mask_2)\n",
    "axe2[1].fill_between(national_df.loc[date_mask_2].resample(\"ME\")[\"Date et Heure\"].mean(), 0.0, (total_autonomy_2 / 1000.0) , color = \"#99CCFF\", alpha = 0.5)\n",
    "axe2[1].set_ylim([-0.1, 0.1])\n",
    "\n",
    "plt.gcf().autofmt_xdate()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Mise en application"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "&#x1F4A5; **To Do**\n",
    "    \n",
    "L'objectif ici est de **comprendre les données** présentes dans le dataset national que vous avez créé. \n",
    "\n",
    "---\n",
    "- **Affichez vos données pour mieux les comprendre** : choisir plusieurs durées pour afficher vos données : semaine, mois, année, durée totale, faîtes des comparaisons... (pensez à utiliser des choses comme rolling mean) \n",
    "    - Tracer l'évolution de la consommation\n",
    "    - Tracer l'évolution de la production par source d'énergie et au niveau global,\n",
    "    - Tracer l'évolution de émissions de CO2.\n",
    "\n",
    "---\n",
    "\n",
    "- **Mix énergétique** :\n",
    "    - Trouvez plusieurs manières de visualiser les données de production : plot, pie chart, rolling mean...\n",
    "    - Quelle est la source de production largement majoritaire ?\n",
    "    - Quelle est l'évolution de la part des énergies fossiles dans le mix énergétique ?  \n",
    "    - Quelle est l'évolution de la part des énergies renouvelables dans le mix énergétique français ?\n",
    "    - Quelle est la part de chaque filière de production d'énergies renouvelables (hydraulique, solaire, ...) \n",
    "\n",
    "---\n",
    "\n",
    "- **Autonomie électrique** :\n",
    "    - comparer la production totale française à la consommation totale. Quelle est le taux d'indépendance énergétique de la france au cours du temps ? C'est à dire la proportion du temps où la France est autonome en énergie. Faire le caclul sur la durée totale mais aussi par année et par mois.\n",
    "    - Trouver les outliers    \n",
    "---\n",
    "\n",
    "- **Equilibre du réseau électrique** : Vérifier que la production, la consommation et les échanges commerciaux s'équilibrent en permanence. <br/> Ressource pour comprendre : https://fr.wikipedia.org/wiki/Ajustement_offre-demande_d%27%C3%A9lectricit%C3%A9 \n",
    "\n",
    "---\n",
    "\n",
    "- **Analyser la tendance des données** :\n",
    "    - Analyser la saisonnalité des productions, comparer les mois de l'année entre eux. Quand consomme-t-on le plus ?\n",
    "    - Printemps vs Eté vs Automne vs Hiver ?\n",
    "    - Quelle est la tendance générale de l'évolution à long terme de la production d'énergie solaire ?\n",
    "    - Quelle est la tendance générale de l'évolution à long terme de l'émission de CO2 ?\n",
    "\n",
    "---\n",
    "\n",
    "- **Impact des sources primaires de production sur le Taux de CO2** :\n",
    "    - Corrélez les données entre elles et déduisez-en l'impact positif ou négatif de chacune des sources de production sur l'estimation du taux de CO2 émis.\n",
    "    <br/> **Pandas** : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html\n",
    "    <br/> **Théorie** : https://data36.com/correlation-definition-calculation-corr-pandas/\n",
    "\n",
    "---\n",
    "- [Bonus] **Analyser les périodicités des données** :\n",
    "    - Réaliser une analyse de Fourier de vos séries temporelles\n",
    "      <br/> **Théorie et pratique** : https://realpython.com/python-scipy-fft/\n",
    "\n",
    "---\n",
    "\n",
    "- **Géopolitique** :\n",
    "    - Visualisez et analysez l'impact du COVID et de la guerre en Ukraine sur le réseau électrique ?\n",
    "\n",
    "---\n",
    "\n",
    "&#x1F4A5; **Aide**\n",
    "- Ressource utile pour vous aider à analyser les données : https://www.statistiques.developpement-durable.gouv.fr/edition-numerique/bilan-energetique-2020/\n",
    "- `pandas.DataFrame.rolling` vous aidera à afficher vos données et en comprendre les tendances. \n",
    "- Les `pandas.Grouper` vous aideront pour grouper les données temporelles.\n",
    "- la méthode `.plot()` fonctionne parfaitement avec un datetime en index\n",
    "- Corrélation : https://pandas.pydata.org/docs/reference/api/pandas.DataFrame.corr.html\n",
    "- attention aux `NaN` pour les plots."
   ]
  }
 ],
 "metadata": {
  "extensions": {
   "jupyter_dashboards": {
    "activeView": "report_default",
    "version": 1,
    "views": {
     "grid_default": {
      "cellMargin": 10,
      "defaultCellHeight": 20,
      "maxColumns": 12,
      "name": "grid",
      "type": "grid"
     },
     "report_default": {
      "name": "report",
      "type": "report"
     }
    }
   }
  },
  "kernelspec": {
   "display_name": "deks",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.2"
  },
  "toc": {
   "base_numbering": 1,
   "nav_menu": {},
   "number_sections": true,
   "sideBar": true,
   "skip_h1_title": true,
   "title_cell": "Table of Contents",
   "title_sidebar": "Contents",
   "toc_cell": false,
   "toc_position": {
    "height": "calc(100% - 180px)",
    "left": "10px",
    "top": "150px",
    "width": "204.067px"
   },
   "toc_section_display": true,
   "toc_window_display": false
  },
  "toc-autonumbering": true,
  "toc-showcode": false,
  "varInspector": {
   "cols": {
    "lenName": 16,
    "lenType": 16,
    "lenVar": 40
   },
   "kernels_config": {
    "python": {
     "delete_cmd_postfix": "",
     "delete_cmd_prefix": "del ",
     "library": "var_list.py",
     "varRefreshCmd": "print(var_dic_list())"
    },
    "r": {
     "delete_cmd_postfix": ") ",
     "delete_cmd_prefix": "rm(",
     "library": "var_list.r",
     "varRefreshCmd": "cat(var_dic_list()) "
    }
   },
   "types_to_exclude": [
    "module",
    "function",
    "builtin_function_or_method",
    "instance",
    "_Feature"
   ],
   "window_display": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
